  0%|                                                                                                                        | 0/900 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 ... (more hidden) ...                                                                                                                               
{'loss': 0.6942, 'grad_norm': 0.5327717661857605, 'learning_rate': 2.2222222222222223e-05, 'epoch': 1.67}
{'train_runtime': 2119.1037, 'train_samples_per_second': 13.565, 'train_steps_per_second': 0.425, 'train_loss': 0.6940929158528646, 'epoch': 3.0}
âœ… Reward model saved!
